{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:51:58.664443Z","iopub.execute_input":"2025-08-20T04:51:58.664921Z","iopub.status.idle":"2025-08-20T04:51:58.672461Z","shell.execute_reply.started":"2025-08-20T04:51:58.664889Z","shell.execute_reply":"2025-08-20T04:51:58.671690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading the data\ntrain=pd.read_csv(\"/kaggle/input/playground-series-s5e8/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/playground-series-s5e8/test.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:51:58.673647Z","iopub.execute_input":"2025-08-20T04:51:58.673916Z","iopub.status.idle":"2025-08-20T04:52:00.819929Z","shell.execute_reply.started":"2025-08-20T04:51:58.673896Z","shell.execute_reply":"2025-08-20T04:52:00.818934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****DATA PREPROCESSING****","metadata":{}},{"cell_type":"code","source":"# Check unique values in the 'job' feature\n\n# Get the number of unique values\nnum_unique_jobs = train['job'].nunique()\n\n# Get the actual unique values\nunique_jobs = train['job'].unique()\n\n# Print results\nprint(\"Number of unique job values:\", num_unique_jobs)\nprint(\"Unique job values:\", unique_jobs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.821211Z","iopub.execute_input":"2025-08-20T04:52:00.821484Z","iopub.status.idle":"2025-08-20T04:52:00.912391Z","shell.execute_reply.started":"2025-08-20T04:52:00.821460Z","shell.execute_reply":"2025-08-20T04:52:00.911557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each job category\njob_counts = train['job'].value_counts()\n\nprint(job_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.913119Z","iopub.execute_input":"2025-08-20T04:52:00.913334Z","iopub.status.idle":"2025-08-20T04:52:00.975586Z","shell.execute_reply.started":"2025-08-20T04:52:00.913316Z","shell.execute_reply":"2025-08-20T04:52:00.974462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group data by 'job' and calculate the mean of the target 'y'\n# This tells us the probability of subscribing (y=1) for each job type\njob_target_mean = train.groupby('job')['y'].mean()\n\n# Sort the job types by the probability of subscription in descending order\njob_target_sorted = job_target_mean.sort_values(ascending=False)\n\n# Print the result\nprint(\"Jobs sorted by likelihood to subscribe to a bank term deposit:\")\nprint(job_target_sorted)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.977613Z","iopub.execute_input":"2025-08-20T04:52:00.977854Z","iopub.status.idle":"2025-08-20T04:52:01.048198Z","shell.execute_reply.started":"2025-08-20T04:52:00.977834Z","shell.execute_reply":"2025-08-20T04:52:01.047410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'job'\n# ----------------------------\n\n# Step 1: Map the sorted subscription probabilities to create 'job_encoded' in training set\ntrain['job_encoded'] = train['job'].map(job_target_sorted)\n\n# Step 2: Apply the same mapping to the test set\n# Always use the mapping from the training set to avoid data leakage\ntest['job_encoded'] = test['job'].map(job_target_sorted)\n\n# Step 3: Check the result\n# Display first 10 rows of original job, encoded value, and target\nprint(train[['job', 'job_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.048917Z","iopub.execute_input":"2025-08-20T04:52:01.049144Z","iopub.status.idle":"2025-08-20T04:52:01.129185Z","shell.execute_reply.started":"2025-08-20T04:52:01.049125Z","shell.execute_reply":"2025-08-20T04:52:01.128349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Step 1: Calculate subscription probability per job\njob_target_mean = train.groupby('job')['y'].mean()\n\n# Step 2: Sort jobs by probability in ascending order (less likely = 0, most likely = highest number)\njob_sorted = job_target_mean.sort_values().index  # returns job names sorted by subscription probability\n\n# Step 3: Create a mapping from job name to integer label\njob_label_mapping = {job: idx for idx, job in enumerate(job_sorted)}\n\n# Step 4: Apply this mapping to train and test sets\ntrain['job_encoded'] = train['job'].map(job_label_mapping)\ntest['job_encoded'] = test['job'].map(job_label_mapping)\n\n# Step 5: Check result\nprint(train[['job', 'job_encoded', 'y']].head(10))\n\n\n\n\noutput:\n\n           job  job_encoded  y\n0   technician            5  0\n1  blue-collar            0  0\n2  blue-collar            0  0\n3      student           11  0\n4   technician            5  1\n5       admin.            4  0\n6  blue-collar            0  0\n7       admin.            4  0\n8  blue-collar            0  0\n9   management            8  0\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.129872Z","iopub.execute_input":"2025-08-20T04:52:01.130107Z","iopub.status.idle":"2025-08-20T04:52:01.135910Z","shell.execute_reply.started":"2025-08-20T04:52:01.130089Z","shell.execute_reply":"2025-08-20T04:52:01.135060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*in target encoding = Captures subtle differences between categories (e.g., 0.118321 vs 0.116453 is meaningful).*\n\n*in lable encoding = blue-collar = 0.067 vs entrepreneur = 0.081 → both get integers 0 and 1, but the real difference is very small.*\n\n*so we choose target encoding *","metadata":{}},{"cell_type":"code","source":"# Get unique values in 'marital' column\nmarital_unique = train['marital'].unique()\nprint(\"Unique marital values:\", marital_unique)\n\n# Get the number of unique values\nmarital_count = train['marital'].nunique()\nprint(\"Number of unique marital values:\", marital_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.136755Z","iopub.execute_input":"2025-08-20T04:52:01.137106Z","iopub.status.idle":"2025-08-20T04:52:01.231813Z","shell.execute_reply.started":"2025-08-20T04:52:01.137079Z","shell.execute_reply":"2025-08-20T04:52:01.231068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'marital'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each marital status\nmarital_target_mean = train.groupby('marital')['y'].mean()\n\n# Step 2: Sort marital statuses by probability of subscribing (optional)\nmarital_target_sorted = marital_target_mean.sort_values(ascending=False)\n\n# Step 3: Map the probabilities to the training set\ntrain['marital_encoded'] = train['marital'].map(marital_target_sorted)\n\n# Step 4: Apply the same mapping to the test set\ntest['marital_encoded'] = test['marital'].map(marital_target_sorted)\n\n# Step 5: Check the result\nprint(train[['marital', 'marital_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.232614Z","iopub.execute_input":"2025-08-20T04:52:01.232836Z","iopub.status.idle":"2025-08-20T04:52:01.373801Z","shell.execute_reply.started":"2025-08-20T04:52:01.232818Z","shell.execute_reply":"2025-08-20T04:52:01.372998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.374694Z","iopub.execute_input":"2025-08-20T04:52:01.375002Z","iopub.status.idle":"2025-08-20T04:52:01.388356Z","shell.execute_reply.started":"2025-08-20T04:52:01.374976Z","shell.execute_reply":"2025-08-20T04:52:01.387570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop original categorical columns after encoding\ntrain = train.drop(columns=['job', 'marital'])\ntest = test.drop(columns=['job', 'marital'])\n\n# Check the first few rows\nprint(train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.390439Z","iopub.execute_input":"2025-08-20T04:52:01.391201Z","iopub.status.idle":"2025-08-20T04:52:01.511146Z","shell.execute_reply.started":"2025-08-20T04:52:01.391177Z","shell.execute_reply":"2025-08-20T04:52:01.510363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''how does this works \n   # Step 1: Calculate mean subscription probability for each marital status\n   marital_target_mean = train.groupby('marital')['y'].mean()\n\n\nExplanation:\n\ntrain.groupby('marital')\n\nThis groups the dataset by the values in the marital column.\n\nFor example, all rows where marital = married are grouped together, all rows with marital = single are another group, etc.\n\n['y']\n\nWe select only the target column y (which indicates whether a client subscribed: 1 = yes, 0 = no).\n\n.mean()\n\nCalculates the average of y for each marital group.\n\nSince y is 0 or 1, the mean is essentially the probability of subscription for that marital status.\n\nExample: if 100 married people are in the dataset and 15 subscribed (y=1), the mean is 15/100 = 0.15.\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get unique values in 'education' column\neducation_unique = train['education'].unique()\nprint(\"Unique education values:\", education_unique)\n\n# Get the number of unique values\neducation_count = train['education'].nunique()\nprint(\"Number of unique education values:\", education_count)\n\n# Optionally, see the frequency of each education level\nprint(train['education'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:09:10.451302Z","iopub.execute_input":"2025-08-20T05:09:10.451582Z","iopub.status.idle":"2025-08-20T05:09:10.594171Z","shell.execute_reply.started":"2025-08-20T05:09:10.451562Z","shell.execute_reply":"2025-08-20T05:09:10.593437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'education'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each education level\neducation_target_mean = train.groupby('education')['y'].mean()\n\n# Step 2: Sort education levels by probability of subscribing (optional)\neducation_target_sorted = education_target_mean.sort_values(ascending=False)\n\n# Step 3: Map the probabilities to the training set\ntrain['education_encoded'] = train['education'].map(education_target_sorted)\n\n# Step 4: Apply the same mapping to the test set\ntest['education_encoded'] = test['education'].map(education_target_sorted)\n\n# Step 5: Check the result\nprint(train[['education', 'education_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:09:46.073533Z","iopub.execute_input":"2025-08-20T05:09:46.074115Z","iopub.status.idle":"2025-08-20T05:09:46.199616Z","shell.execute_reply.started":"2025-08-20T05:09:46.074091Z","shell.execute_reply":"2025-08-20T05:09:46.198852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'default' column\ndefault_unique = train['default'].unique()\nprint(\"Unique default values:\", default_unique)\n\n# Number of unique values\ndefault_count = train['default'].nunique()\nprint(\"Number of unique default values:\", default_count)\n\n# Frequency of each value\nprint(train['default'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:10:37.952631Z","iopub.execute_input":"2025-08-20T05:10:37.953206Z","iopub.status.idle":"2025-08-20T05:10:38.076424Z","shell.execute_reply.started":"2025-08-20T05:10:37.953177Z","shell.execute_reply":"2025-08-20T05:10:38.075549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple label encoding for 'default'\ntrain['default_encoded'] = train['default'].map({'no': 0, 'yes': 1})\ntest['default_encoded'] = test['default'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['default'])\ntest = test.drop(columns=['default'])\n\n# Check the result\nprint(train[['default_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:11:22.997266Z","iopub.execute_input":"2025-08-20T05:11:22.997827Z","iopub.status.idle":"2025-08-20T05:11:23.164801Z","shell.execute_reply.started":"2025-08-20T05:11:22.997802Z","shell.execute_reply":"2025-08-20T05:11:23.164081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the original 'education' column after encoding\ntrain = train.drop(columns=['education'])\ntest = test.drop(columns=['education'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:12:00.930949Z","iopub.execute_input":"2025-08-20T05:12:00.931283Z","iopub.status.idle":"2025-08-20T05:12:01.016579Z","shell.execute_reply.started":"2025-08-20T05:12:00.931261Z","shell.execute_reply":"2025-08-20T05:12:01.015873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'balance'\nbalance_unique_count = train['balance'].nunique()\nprint(\"Number of unique balance values:\", balance_unique_count)\n\n# List of unique values (optional, might be very long)\nbalance_unique_values = train['balance'].unique()\n#print(\"Unique balance values:\", balance_unique_values)\n\n# Number of missing/null values in 'balance'\nbalance_null_count = train['balance'].isnull().sum()\nprint(\"Number of null values in balance:\", balance_null_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:16:31.438053Z","iopub.execute_input":"2025-08-20T05:16:31.438796Z","iopub.status.idle":"2025-08-20T05:16:31.461188Z","shell.execute_reply.started":"2025-08-20T05:16:31.438749Z","shell.execute_reply":"2025-08-20T05:16:31.460344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'housing'\nhousing_unique = train['housing'].unique()\nprint(\"Unique housing values:\", housing_unique)\n\n# Number of unique values\nprint(\"Number of unique housing values:\", train['housing'].nunique())\n\n# Frequency of each value\nprint(train['housing'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:23:31.826275Z","iopub.execute_input":"2025-08-20T05:23:31.826566Z","iopub.status.idle":"2025-08-20T05:23:31.960175Z","shell.execute_reply.started":"2025-08-20T05:23:31.826544Z","shell.execute_reply":"2025-08-20T05:23:31.959316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple label encoding for 'housing'\ntrain['housing_encoded'] = train['housing'].map({'no': 0, 'yes': 1})\ntest['housing_encoded'] = test['housing'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['housing'])\ntest = test.drop(columns=['housing'])\n\n# Check result\nprint(train[['housing_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:23:45.795108Z","iopub.execute_input":"2025-08-20T05:23:45.795980Z","iopub.status.idle":"2025-08-20T05:23:45.968367Z","shell.execute_reply.started":"2025-08-20T05:23:45.795944Z","shell.execute_reply":"2025-08-20T05:23:45.967521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'loan'\nloan_unique = train['loan'].unique()\nprint(\"Unique loan values:\", loan_unique)\n\n# Number of unique values\nprint(\"Number of unique loan values:\", train['loan'].nunique())\n\n# Frequency of each value\nprint(train['loan'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:07.142952Z","iopub.execute_input":"2025-08-20T05:25:07.143281Z","iopub.status.idle":"2025-08-20T05:25:07.269940Z","shell.execute_reply.started":"2025-08-20T05:25:07.143259Z","shell.execute_reply":"2025-08-20T05:25:07.269185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Label encoding for 'loan'\ntrain['loan_encoded'] = train['loan'].map({'no': 0, 'yes': 1})\ntest['loan_encoded'] = test['loan'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['loan'])\ntest = test.drop(columns=['loan'])\n\n# Check first few rows\nprint(train[['loan_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:19.618611Z","iopub.execute_input":"2025-08-20T05:25:19.619227Z","iopub.status.idle":"2025-08-20T05:25:19.760409Z","shell.execute_reply.started":"2025-08-20T05:25:19.619201Z","shell.execute_reply":"2025-08-20T05:25:19.759585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'contact'\ncontact_unique = train['contact'].unique()\nprint(\"Unique contact values:\", contact_unique)\n\n# Number of unique values\nprint(\"Number of unique contact values:\", train['contact'].nunique())\n\n# Frequency of each value\nprint(train['contact'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:40.649670Z","iopub.execute_input":"2025-08-20T05:25:40.650347Z","iopub.status.idle":"2025-08-20T05:25:40.788055Z","shell.execute_reply.started":"2025-08-20T05:25:40.650320Z","shell.execute_reply":"2025-08-20T05:25:40.787253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode 'contact' column\ntrain_contact_ohe = pd.get_dummies(train['contact'], prefix='contact')\ntest_contact_ohe = pd.get_dummies(test['contact'], prefix='contact')\n\n# Align columns of train and test (in case some category is missing in test)\ntrain_contact_ohe, test_contact_ohe = train_contact_ohe.align(test_contact_ohe, join='outer', axis=1, fill_value=0)\n\n# Add one-hot columns to original dataset\ntrain = pd.concat([train, train_contact_ohe], axis=1)\ntest = pd.concat([test, test_contact_ohe], axis=1)\n\n# Optional: drop original 'contact' column\ntrain = train.drop(columns=['contact'])\ntest = test.drop(columns=['contact'])\n\n# Check first few rows\nprint(train.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:26:53.948848Z","iopub.execute_input":"2025-08-20T05:26:53.949162Z","iopub.status.idle":"2025-08-20T05:26:54.229314Z","shell.execute_reply.started":"2025-08-20T05:26:53.949140Z","shell.execute_reply":"2025-08-20T05:26:54.228514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'day'\nday_unique = train['day'].unique()\nprint(\"Unique day values:\", day_unique)\n\n# Number of unique values\nprint(\"Number of unique day values:\", train['day'].nunique())\n\n# Frequency of each value\nprint(train['day'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:27:23.713957Z","iopub.execute_input":"2025-08-20T05:27:23.714282Z","iopub.status.idle":"2025-08-20T05:27:23.736974Z","shell.execute_reply.started":"2025-08-20T05:27:23.714261Z","shell.execute_reply":"2025-08-20T05:27:23.736228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display first 10 rows of 'day' column\nprint(train['day'].head(10))\n\n# Or display the entire column (careful if dataset is large)\nprint(train['day'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:28:07.648392Z","iopub.execute_input":"2025-08-20T05:28:07.648651Z","iopub.status.idle":"2025-08-20T05:28:07.654957Z","shell.execute_reply.started":"2025-08-20T05:28:07.648632Z","shell.execute_reply":"2025-08-20T05:28:07.653984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total number of rows in the training set\ntotal_rows = train.shape[0]\nprint(\"Total number of rows:\", total_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:32:22.750988Z","iopub.execute_input":"2025-08-20T05:32:22.751313Z","iopub.status.idle":"2025-08-20T05:32:22.756115Z","shell.execute_reply.started":"2025-08-20T05:32:22.751289Z","shell.execute_reply":"2025-08-20T05:32:22.755218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'month'\nmonth_unique = train['month'].unique()\nprint(\"Unique month values:\", month_unique)\n\n# Number of unique month values\nprint(\"Number of unique month values:\", train['month'].nunique())\n\n# Frequency of each month\nprint(train['month'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:32:55.039791Z","iopub.execute_input":"2025-08-20T05:32:55.040127Z","iopub.status.idle":"2025-08-20T05:32:55.169863Z","shell.execute_reply.started":"2025-08-20T05:32:55.040102Z","shell.execute_reply":"2025-08-20T05:32:55.169076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What is Cyclical Encoding?\n\nSome features are cyclical, meaning the first and last values are close to each other in meaning:\n\nMonths: Dec → Jan\n\nHours: 23 → 0\n\nDays of the week: Sun → Mon\n\nIf you treat them as numeric (1–12 for months), the model might think Dec (12) is far from Jan (1), which is misleading.\n\nCyclical encoding fixes this using sine and cosine transformations:","metadata":{}},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'month'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each month\nmonth_target_mean = train.groupby('month')['y'].mean()\n\n# Step 2: Sort months by subscription probability (optional, just for checking)\nmonth_target_sorted = month_target_mean.sort_values(ascending=False)\nprint(\"Months sorted by likelihood to subscribe:\")\nprint(month_target_sorted)\n\n# Step 3: Map the target mean to create 'month_encoded' in training set\ntrain['month_encoded'] = train['month'].map(month_target_mean)\n\n# Step 4: Apply the same mapping to the test set\n# Always use the mapping from the training set to avoid data leakage\ntest['month_encoded'] = test['month'].map(month_target_mean)\n\n# Step 5: Check the result\nprint(train[['month', 'month_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:59:53.606843Z","iopub.execute_input":"2025-08-20T05:59:53.607161Z","iopub.status.idle":"2025-08-20T05:59:53.751587Z","shell.execute_reply.started":"2025-08-20T05:59:53.607138Z","shell.execute_reply":"2025-08-20T05:59:53.750713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check basic stats of 'duration'\nprint(train['duration'].describe())\n\n# Check for unique values count\nprint(\"Number of unique duration values:\", train['duration'].nunique())\n\n# Optional: see top 10 values\nprint(train['duration'].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:00:06.073467Z","iopub.execute_input":"2025-08-20T06:00:06.073735Z","iopub.status.idle":"2025-08-20T06:00:06.160703Z","shell.execute_reply.started":"2025-08-20T06:00:06.073717Z","shell.execute_reply":"2025-08-20T06:00:06.159947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'campaign'\nprint(\"Number of unique campaign values:\", train['campaign'].nunique())\n\n# Display the unique values themselves\nprint(\"Unique campaign values:\", train['campaign'].unique())\n\n# Optional: Get value counts to see frequency of each number\nprint(\"Value counts for 'campaign':\")\nprint(train['campaign'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:01:21.353703Z","iopub.execute_input":"2025-08-20T06:01:21.354476Z","iopub.status.idle":"2025-08-20T06:01:21.379447Z","shell.execute_reply.started":"2025-08-20T06:01:21.354444Z","shell.execute_reply":"2025-08-20T06:01:21.378619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check basic stats of 'pdays'\nprint(train['pdays'].describe())\n\n# Number of unique values\nprint(\"Number of unique pdays values:\", train['pdays'].nunique())\n\n# Unique values themselves (optional, might be a lot)\nprint(\"Unique pdays values (sample):\", train['pdays'].unique()[:20])\n\n# Value counts to see frequency of each value\nprint(\"Value counts for 'pdays':\")\nprint(train['pdays'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:02:17.714083Z","iopub.execute_input":"2025-08-20T06:02:17.714351Z","iopub.status.idle":"2025-08-20T06:02:17.755172Z","shell.execute_reply.started":"2025-08-20T06:02:17.714333Z","shell.execute_reply":"2025-08-20T06:02:17.754306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'previous'\nprint(\"Number of unique values in 'previous':\", train['previous'].nunique())\n\n# Display the unique values themselves\nprint(\"Unique 'previous' values:\", train['previous'].unique())\n\n# Value counts to see how often each number occurs\nprint(\"Value counts for 'previous':\")\nprint(train['previous'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:03:33.789916Z","iopub.execute_input":"2025-08-20T06:03:33.790252Z","iopub.status.idle":"2025-08-20T06:03:33.812892Z","shell.execute_reply.started":"2025-08-20T06:03:33.790228Z","shell.execute_reply":"2025-08-20T06:03:33.811960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unique values in 'poutcome'\nprint(\"Unique 'poutcome' values:\", train['poutcome'].unique())\n\n# Number of unique values\nprint(\"Number of unique 'poutcome' values:\", train['poutcome'].nunique())\n\n# Value counts to see distribution\nprint(\"Value counts for 'poutcome':\")\nprint(train['poutcome'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:04:31.496493Z","iopub.execute_input":"2025-08-20T06:04:31.496776Z","iopub.status.idle":"2025-08-20T06:04:31.625980Z","shell.execute_reply.started":"2025-08-20T06:04:31.496750Z","shell.execute_reply":"2025-08-20T06:04:31.625211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform one-hot encoding for 'poutcome' in the training set\npoutcome_dummies_train = pd.get_dummies(train['poutcome'], prefix='poutcome')\n\n# Concatenate the new one-hot columns to the original training dataframe\ntrain = pd.concat([train, poutcome_dummies_train], axis=1)\n\n# Do the same for the test set\npoutcome_dummies_test = pd.get_dummies(test['poutcome'], prefix='poutcome')\ntest = pd.concat([test, poutcome_dummies_test], axis=1)\n\n# Optional: Drop the original 'poutcome' column if you don't need it anymore\ntrain.drop('poutcome', axis=1, inplace=True)\ntest.drop('poutcome', axis=1, inplace=True)\n\n# Check the first few rows to see the one-hot encoded columns\nprint(train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:05:08.261541Z","iopub.execute_input":"2025-08-20T06:05:08.261834Z","iopub.status.idle":"2025-08-20T06:05:08.708098Z","shell.execute_reply.started":"2025-08-20T06:05:08.261804Z","shell.execute_reply":"2025-08-20T06:05:08.707176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:05:22.840928Z","iopub.execute_input":"2025-08-20T06:05:22.841612Z","iopub.status.idle":"2025-08-20T06:05:22.859227Z","shell.execute_reply.started":"2025-08-20T06:05:22.841582Z","shell.execute_reply":"2025-08-20T06:05:22.858270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Columns in the training set\nprint(\"Training set columns:\")\nprint(train.columns)\n\n# Columns in the test set\nprint(\"\\nTest set columns:\")\nprint(test.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:06:30.031595Z","iopub.execute_input":"2025-08-20T06:06:30.031870Z","iopub.status.idle":"2025-08-20T06:06:30.037261Z","shell.execute_reply.started":"2025-08-20T06:06:30.031849Z","shell.execute_reply":"2025-08-20T06:06:30.036313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List all column names in the training dataset\nprint(train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:08:20.689277Z","iopub.execute_input":"2025-08-20T06:08:20.689558Z","iopub.status.idle":"2025-08-20T06:08:20.694590Z","shell.execute_reply.started":"2025-08-20T06:08:20.689534Z","shell.execute_reply":"2025-08-20T06:08:20.693582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the original 'month' column since 'month_encoded' exists\ntrain.drop(columns=['month'], inplace=True)\ntest.drop(columns=['month'], inplace=True)\n\n# Check remaining columns\nprint(train.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:10:47.923072Z","iopub.execute_input":"2025-08-20T06:10:47.923665Z","iopub.status.idle":"2025-08-20T06:10:47.983382Z","shell.execute_reply.started":"2025-08-20T06:10:47.923639Z","shell.execute_reply":"2025-08-20T06:10:47.982524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate features and target\nX_train = train.drop(columns=['id', 'y'])\ny_train = train['y']\n\n# For test set, drop 'id' column only\nX_test = test.drop(columns=['id'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:13:24.321198Z","iopub.execute_input":"2025-08-20T06:13:24.321909Z","iopub.status.idle":"2025-08-20T06:13:24.569581Z","shell.execute_reply.started":"2025-08-20T06:13:24.321883Z","shell.execute_reply":"2025-08-20T06:13:24.568752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# List of numeric columns to scale\nnumeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit on training data and transform\nX_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n\n# Transform the test set using the same scaler\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\n# Check first 5 rows after scaling\nprint(X_train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:15:44.814537Z","iopub.execute_input":"2025-08-20T06:15:44.815303Z","iopub.status.idle":"2025-08-20T06:15:45.713107Z","shell.execute_reply.started":"2025-08-20T06:15:44.815277Z","shell.execute_reply":"2025-08-20T06:15:45.712193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n# Step 1: Initialize the model\nmodel = LogisticRegression(max_iter=1000, random_state=42)\n\n# Step 2: Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Step 3: Predict probabilities on the training set (to check ROC-AUC)\ny_train_pred_proba = model.predict_proba(X_train)[:, 1]\n\n# Step 4: Calculate ROC-AUC score on training set\nroc_auc = roc_auc_score(y_train, y_train_pred_proba)\nprint(f\"Training ROC-AUC Score: {roc_auc:.4f}\")\n\n# Step 5: Predict probabilities on the test set for submission\ny_test_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# Step 6: Create submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'y': y_test_pred_proba\n})\n\n# Step 7: Save submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:18:32.832695Z","iopub.execute_input":"2025-08-20T06:18:32.833813Z","iopub.status.idle":"2025-08-20T06:19:03.146536Z","shell.execute_reply.started":"2025-08-20T06:18:32.833779Z","shell.execute_reply":"2025-08-20T06:19:03.145760Z"}},"outputs":[],"execution_count":null}]}