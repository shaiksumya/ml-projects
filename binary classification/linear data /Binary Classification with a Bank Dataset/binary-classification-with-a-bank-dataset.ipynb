{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91719,"databundleVersionId":12937777,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:51:58.664443Z","iopub.execute_input":"2025-08-20T04:51:58.664921Z","iopub.status.idle":"2025-08-20T04:51:58.672461Z","shell.execute_reply.started":"2025-08-20T04:51:58.664889Z","shell.execute_reply":"2025-08-20T04:51:58.671690Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading the data\ntrain=pd.read_csv(\"/kaggle/input/playground-series-s5e8/train.csv\")\ntest=pd.read_csv(\"/kaggle/input/playground-series-s5e8/test.csv\")\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:51:58.673647Z","iopub.execute_input":"2025-08-20T04:51:58.673916Z","iopub.status.idle":"2025-08-20T04:52:00.819929Z","shell.execute_reply.started":"2025-08-20T04:51:58.673896Z","shell.execute_reply":"2025-08-20T04:52:00.818934Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"****DATA PREPROCESSING****","metadata":{}},{"cell_type":"code","source":"# Check unique values in the 'job' feature\n\n# Get the number of unique values\nnum_unique_jobs = train['job'].nunique()\n\n# Get the actual unique values\nunique_jobs = train['job'].unique()\n\n# Print results\nprint(\"Number of unique job values:\", num_unique_jobs)\nprint(\"Unique job values:\", unique_jobs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.821211Z","iopub.execute_input":"2025-08-20T04:52:00.821484Z","iopub.status.idle":"2025-08-20T04:52:00.912391Z","shell.execute_reply.started":"2025-08-20T04:52:00.821460Z","shell.execute_reply":"2025-08-20T04:52:00.911557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count the frequency of each job category\njob_counts = train['job'].value_counts()\n\nprint(job_counts)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.913119Z","iopub.execute_input":"2025-08-20T04:52:00.913334Z","iopub.status.idle":"2025-08-20T04:52:00.975586Z","shell.execute_reply.started":"2025-08-20T04:52:00.913316Z","shell.execute_reply":"2025-08-20T04:52:00.974462Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Group data by 'job' and calculate the mean of the target 'y'\n# This tells us the probability of subscribing (y=1) for each job type\njob_target_mean = train.groupby('job')['y'].mean()\n\n# Sort the job types by the probability of subscription in descending order\njob_target_sorted = job_target_mean.sort_values(ascending=False)\n\n# Print the result\nprint(\"Jobs sorted by likelihood to subscribe to a bank term deposit:\")\nprint(job_target_sorted)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:00.977613Z","iopub.execute_input":"2025-08-20T04:52:00.977854Z","iopub.status.idle":"2025-08-20T04:52:01.048198Z","shell.execute_reply.started":"2025-08-20T04:52:00.977834Z","shell.execute_reply":"2025-08-20T04:52:01.047410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'job'\n# ----------------------------\n\n# Step 1: Map the sorted subscription probabilities to create 'job_encoded' in training set\ntrain['job_encoded'] = train['job'].map(job_target_sorted)\n\n# Step 2: Apply the same mapping to the test set\n# Always use the mapping from the training set to avoid data leakage\ntest['job_encoded'] = test['job'].map(job_target_sorted)\n\n# Step 3: Check the result\n# Display first 10 rows of original job, encoded value, and target\nprint(train[['job', 'job_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.048917Z","iopub.execute_input":"2025-08-20T04:52:01.049144Z","iopub.status.idle":"2025-08-20T04:52:01.129185Z","shell.execute_reply.started":"2025-08-20T04:52:01.049125Z","shell.execute_reply":"2025-08-20T04:52:01.128349Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# Step 1: Calculate subscription probability per job\njob_target_mean = train.groupby('job')['y'].mean()\n\n# Step 2: Sort jobs by probability in ascending order (less likely = 0, most likely = highest number)\njob_sorted = job_target_mean.sort_values().index  # returns job names sorted by subscription probability\n\n# Step 3: Create a mapping from job name to integer label\njob_label_mapping = {job: idx for idx, job in enumerate(job_sorted)}\n\n# Step 4: Apply this mapping to train and test sets\ntrain['job_encoded'] = train['job'].map(job_label_mapping)\ntest['job_encoded'] = test['job'].map(job_label_mapping)\n\n# Step 5: Check result\nprint(train[['job', 'job_encoded', 'y']].head(10))\n\n\n\n\noutput:\n\n           job  job_encoded  y\n0   technician            5  0\n1  blue-collar            0  0\n2  blue-collar            0  0\n3      student           11  0\n4   technician            5  1\n5       admin.            4  0\n6  blue-collar            0  0\n7       admin.            4  0\n8  blue-collar            0  0\n9   management            8  0\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.129872Z","iopub.execute_input":"2025-08-20T04:52:01.130107Z","iopub.status.idle":"2025-08-20T04:52:01.135910Z","shell.execute_reply.started":"2025-08-20T04:52:01.130089Z","shell.execute_reply":"2025-08-20T04:52:01.135060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*in target encoding = Captures subtle differences between categories (e.g., 0.118321 vs 0.116453 is meaningful).*\n\n*in lable encoding = blue-collar = 0.067 vs entrepreneur = 0.081 â†’ both get integers 0 and 1, but the real difference is very small.*\n\n*so we choose target encoding *","metadata":{}},{"cell_type":"code","source":"# Get unique values in 'marital' column\nmarital_unique = train['marital'].unique()\nprint(\"Unique marital values:\", marital_unique)\n\n# Get the number of unique values\nmarital_count = train['marital'].nunique()\nprint(\"Number of unique marital values:\", marital_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.136755Z","iopub.execute_input":"2025-08-20T04:52:01.137106Z","iopub.status.idle":"2025-08-20T04:52:01.231813Z","shell.execute_reply.started":"2025-08-20T04:52:01.137079Z","shell.execute_reply":"2025-08-20T04:52:01.231068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'marital'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each marital status\nmarital_target_mean = train.groupby('marital')['y'].mean()\n\n# Step 2: Sort marital statuses by probability of subscribing (optional)\nmarital_target_sorted = marital_target_mean.sort_values(ascending=False)\n\n# Step 3: Map the probabilities to the training set\ntrain['marital_encoded'] = train['marital'].map(marital_target_sorted)\n\n# Step 4: Apply the same mapping to the test set\ntest['marital_encoded'] = test['marital'].map(marital_target_sorted)\n\n# Step 5: Check the result\nprint(train[['marital', 'marital_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.232614Z","iopub.execute_input":"2025-08-20T04:52:01.232836Z","iopub.status.idle":"2025-08-20T04:52:01.373801Z","shell.execute_reply.started":"2025-08-20T04:52:01.232818Z","shell.execute_reply":"2025-08-20T04:52:01.372998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.374694Z","iopub.execute_input":"2025-08-20T04:52:01.375002Z","iopub.status.idle":"2025-08-20T04:52:01.388356Z","shell.execute_reply.started":"2025-08-20T04:52:01.374976Z","shell.execute_reply":"2025-08-20T04:52:01.387570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop original categorical columns after encoding\ntrain = train.drop(columns=['job', 'marital'])\ntest = test.drop(columns=['job', 'marital'])\n\n# Check the first few rows\nprint(train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T04:52:01.390439Z","iopub.execute_input":"2025-08-20T04:52:01.391201Z","iopub.status.idle":"2025-08-20T04:52:01.511146Z","shell.execute_reply.started":"2025-08-20T04:52:01.391177Z","shell.execute_reply":"2025-08-20T04:52:01.510363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''how does this works \n   # Step 1: Calculate mean subscription probability for each marital status\n   marital_target_mean = train.groupby('marital')['y'].mean()\n\n\nExplanation:\n\ntrain.groupby('marital')\n\nThis groups the dataset by the values in the marital column.\n\nFor example, all rows where marital = married are grouped together, all rows with marital = single are another group, etc.\n\n['y']\n\nWe select only the target column y (which indicates whether a client subscribed: 1 = yes, 0 = no).\n\n.mean()\n\nCalculates the average of y for each marital group.\n\nSince y is 0 or 1, the mean is essentially the probability of subscription for that marital status.\n\nExample: if 100 married people are in the dataset and 15 subscribed (y=1), the mean is 15/100 = 0.15.\n\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get unique values in 'education' column\neducation_unique = train['education'].unique()\nprint(\"Unique education values:\", education_unique)\n\n# Get the number of unique values\neducation_count = train['education'].nunique()\nprint(\"Number of unique education values:\", education_count)\n\n# Optionally, see the frequency of each education level\nprint(train['education'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:09:10.451302Z","iopub.execute_input":"2025-08-20T05:09:10.451582Z","iopub.status.idle":"2025-08-20T05:09:10.594171Z","shell.execute_reply.started":"2025-08-20T05:09:10.451562Z","shell.execute_reply":"2025-08-20T05:09:10.593437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'education'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each education level\neducation_target_mean = train.groupby('education')['y'].mean()\n\n# Step 2: Sort education levels by probability of subscribing (optional)\neducation_target_sorted = education_target_mean.sort_values(ascending=False)\n\n# Step 3: Map the probabilities to the training set\ntrain['education_encoded'] = train['education'].map(education_target_sorted)\n\n# Step 4: Apply the same mapping to the test set\ntest['education_encoded'] = test['education'].map(education_target_sorted)\n\n# Step 5: Check the result\nprint(train[['education', 'education_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:09:46.073533Z","iopub.execute_input":"2025-08-20T05:09:46.074115Z","iopub.status.idle":"2025-08-20T05:09:46.199616Z","shell.execute_reply.started":"2025-08-20T05:09:46.074091Z","shell.execute_reply":"2025-08-20T05:09:46.198852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'default' column\ndefault_unique = train['default'].unique()\nprint(\"Unique default values:\", default_unique)\n\n# Number of unique values\ndefault_count = train['default'].nunique()\nprint(\"Number of unique default values:\", default_count)\n\n# Frequency of each value\nprint(train['default'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:10:37.952631Z","iopub.execute_input":"2025-08-20T05:10:37.953206Z","iopub.status.idle":"2025-08-20T05:10:38.076424Z","shell.execute_reply.started":"2025-08-20T05:10:37.953177Z","shell.execute_reply":"2025-08-20T05:10:38.075549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple label encoding for 'default'\ntrain['default_encoded'] = train['default'].map({'no': 0, 'yes': 1})\ntest['default_encoded'] = test['default'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['default'])\ntest = test.drop(columns=['default'])\n\n# Check the result\nprint(train[['default_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:11:22.997266Z","iopub.execute_input":"2025-08-20T05:11:22.997827Z","iopub.status.idle":"2025-08-20T05:11:23.164801Z","shell.execute_reply.started":"2025-08-20T05:11:22.997802Z","shell.execute_reply":"2025-08-20T05:11:23.164081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the original 'education' column after encoding\ntrain = train.drop(columns=['education'])\ntest = test.drop(columns=['education'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:12:00.930949Z","iopub.execute_input":"2025-08-20T05:12:00.931283Z","iopub.status.idle":"2025-08-20T05:12:01.016579Z","shell.execute_reply.started":"2025-08-20T05:12:00.931261Z","shell.execute_reply":"2025-08-20T05:12:01.015873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'balance'\nbalance_unique_count = train['balance'].nunique()\nprint(\"Number of unique balance values:\", balance_unique_count)\n\n# List of unique values (optional, might be very long)\nbalance_unique_values = train['balance'].unique()\n#print(\"Unique balance values:\", balance_unique_values)\n\n# Number of missing/null values in 'balance'\nbalance_null_count = train['balance'].isnull().sum()\nprint(\"Number of null values in balance:\", balance_null_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:16:31.438053Z","iopub.execute_input":"2025-08-20T05:16:31.438796Z","iopub.status.idle":"2025-08-20T05:16:31.461188Z","shell.execute_reply.started":"2025-08-20T05:16:31.438749Z","shell.execute_reply":"2025-08-20T05:16:31.460344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'housing'\nhousing_unique = train['housing'].unique()\nprint(\"Unique housing values:\", housing_unique)\n\n# Number of unique values\nprint(\"Number of unique housing values:\", train['housing'].nunique())\n\n# Frequency of each value\nprint(train['housing'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:23:31.826275Z","iopub.execute_input":"2025-08-20T05:23:31.826566Z","iopub.status.idle":"2025-08-20T05:23:31.960175Z","shell.execute_reply.started":"2025-08-20T05:23:31.826544Z","shell.execute_reply":"2025-08-20T05:23:31.959316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple label encoding for 'housing'\ntrain['housing_encoded'] = train['housing'].map({'no': 0, 'yes': 1})\ntest['housing_encoded'] = test['housing'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['housing'])\ntest = test.drop(columns=['housing'])\n\n# Check result\nprint(train[['housing_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:23:45.795108Z","iopub.execute_input":"2025-08-20T05:23:45.795980Z","iopub.status.idle":"2025-08-20T05:23:45.968367Z","shell.execute_reply.started":"2025-08-20T05:23:45.795944Z","shell.execute_reply":"2025-08-20T05:23:45.967521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'loan'\nloan_unique = train['loan'].unique()\nprint(\"Unique loan values:\", loan_unique)\n\n# Number of unique values\nprint(\"Number of unique loan values:\", train['loan'].nunique())\n\n# Frequency of each value\nprint(train['loan'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:07.142952Z","iopub.execute_input":"2025-08-20T05:25:07.143281Z","iopub.status.idle":"2025-08-20T05:25:07.269940Z","shell.execute_reply.started":"2025-08-20T05:25:07.143259Z","shell.execute_reply":"2025-08-20T05:25:07.269185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Label encoding for 'loan'\ntrain['loan_encoded'] = train['loan'].map({'no': 0, 'yes': 1})\ntest['loan_encoded'] = test['loan'].map({'no': 0, 'yes': 1})\n\n# Optional: drop original column\ntrain = train.drop(columns=['loan'])\ntest = test.drop(columns=['loan'])\n\n# Check first few rows\nprint(train[['loan_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:19.618611Z","iopub.execute_input":"2025-08-20T05:25:19.619227Z","iopub.status.idle":"2025-08-20T05:25:19.760409Z","shell.execute_reply.started":"2025-08-20T05:25:19.619201Z","shell.execute_reply":"2025-08-20T05:25:19.759585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'contact'\ncontact_unique = train['contact'].unique()\nprint(\"Unique contact values:\", contact_unique)\n\n# Number of unique values\nprint(\"Number of unique contact values:\", train['contact'].nunique())\n\n# Frequency of each value\nprint(train['contact'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:25:40.649670Z","iopub.execute_input":"2025-08-20T05:25:40.650347Z","iopub.status.idle":"2025-08-20T05:25:40.788055Z","shell.execute_reply.started":"2025-08-20T05:25:40.650320Z","shell.execute_reply":"2025-08-20T05:25:40.787253Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# One-hot encode 'contact' column\ntrain_contact_ohe = pd.get_dummies(train['contact'], prefix='contact')\ntest_contact_ohe = pd.get_dummies(test['contact'], prefix='contact')\n\n# Align columns of train and test (in case some category is missing in test)\ntrain_contact_ohe, test_contact_ohe = train_contact_ohe.align(test_contact_ohe, join='outer', axis=1, fill_value=0)\n\n# Add one-hot columns to original dataset\ntrain = pd.concat([train, train_contact_ohe], axis=1)\ntest = pd.concat([test, test_contact_ohe], axis=1)\n\n# Optional: drop original 'contact' column\ntrain = train.drop(columns=['contact'])\ntest = test.drop(columns=['contact'])\n\n# Check first few rows\nprint(train.head(5))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:26:53.948848Z","iopub.execute_input":"2025-08-20T05:26:53.949162Z","iopub.status.idle":"2025-08-20T05:26:54.229314Z","shell.execute_reply.started":"2025-08-20T05:26:53.949140Z","shell.execute_reply":"2025-08-20T05:26:54.228514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'day'\nday_unique = train['day'].unique()\nprint(\"Unique day values:\", day_unique)\n\n# Number of unique values\nprint(\"Number of unique day values:\", train['day'].nunique())\n\n# Frequency of each value\nprint(train['day'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:27:23.713957Z","iopub.execute_input":"2025-08-20T05:27:23.714282Z","iopub.status.idle":"2025-08-20T05:27:23.736974Z","shell.execute_reply.started":"2025-08-20T05:27:23.714261Z","shell.execute_reply":"2025-08-20T05:27:23.736228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display first 10 rows of 'day' column\nprint(train['day'].head(10))\n\n# Or display the entire column (careful if dataset is large)\nprint(train['day'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:28:07.648392Z","iopub.execute_input":"2025-08-20T05:28:07.648651Z","iopub.status.idle":"2025-08-20T05:28:07.654957Z","shell.execute_reply.started":"2025-08-20T05:28:07.648632Z","shell.execute_reply":"2025-08-20T05:28:07.653984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Total number of rows in the training set\ntotal_rows = train.shape[0]\nprint(\"Total number of rows:\", total_rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:32:22.750988Z","iopub.execute_input":"2025-08-20T05:32:22.751313Z","iopub.status.idle":"2025-08-20T05:32:22.756115Z","shell.execute_reply.started":"2025-08-20T05:32:22.751289Z","shell.execute_reply":"2025-08-20T05:32:22.755218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Unique values in 'month'\nmonth_unique = train['month'].unique()\nprint(\"Unique month values:\", month_unique)\n\n# Number of unique month values\nprint(\"Number of unique month values:\", train['month'].nunique())\n\n# Frequency of each month\nprint(train['month'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:32:55.039791Z","iopub.execute_input":"2025-08-20T05:32:55.040127Z","iopub.status.idle":"2025-08-20T05:32:55.169863Z","shell.execute_reply.started":"2025-08-20T05:32:55.040102Z","shell.execute_reply":"2025-08-20T05:32:55.169076Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"What is Cyclical Encoding?\n\nSome features are cyclical, meaning the first and last values are close to each other in meaning:\n\nMonths: Dec â†’ Jan\n\nHours: 23 â†’ 0\n\nDays of the week: Sun â†’ Mon\n\nIf you treat them as numeric (1â€“12 for months), the model might think Dec (12) is far from Jan (1), which is misleading.\n\nCyclical encoding fixes this using sine and cosine transformations:","metadata":{}},{"cell_type":"code","source":"# ----------------------------\n# TARGET ENCODING FOR 'month'\n# ----------------------------\n\n# Step 1: Calculate mean subscription probability for each month\nmonth_target_mean = train.groupby('month')['y'].mean()\n\n# Step 2: Sort months by subscription probability (optional, just for checking)\nmonth_target_sorted = month_target_mean.sort_values(ascending=False)\nprint(\"Months sorted by likelihood to subscribe:\")\nprint(month_target_sorted)\n\n# Step 3: Map the target mean to create 'month_encoded' in training set\ntrain['month_encoded'] = train['month'].map(month_target_mean)\n\n# Step 4: Apply the same mapping to the test set\n# Always use the mapping from the training set to avoid data leakage\ntest['month_encoded'] = test['month'].map(month_target_mean)\n\n# Step 5: Check the result\nprint(train[['month', 'month_encoded', 'y']].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T05:59:53.606843Z","iopub.execute_input":"2025-08-20T05:59:53.607161Z","iopub.status.idle":"2025-08-20T05:59:53.751587Z","shell.execute_reply.started":"2025-08-20T05:59:53.607138Z","shell.execute_reply":"2025-08-20T05:59:53.750713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check basic stats of 'duration'\nprint(train['duration'].describe())\n\n# Check for unique values count\nprint(\"Number of unique duration values:\", train['duration'].nunique())\n\n# Optional: see top 10 values\nprint(train['duration'].head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:00:06.073467Z","iopub.execute_input":"2025-08-20T06:00:06.073735Z","iopub.status.idle":"2025-08-20T06:00:06.160703Z","shell.execute_reply.started":"2025-08-20T06:00:06.073717Z","shell.execute_reply":"2025-08-20T06:00:06.159947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'campaign'\nprint(\"Number of unique campaign values:\", train['campaign'].nunique())\n\n# Display the unique values themselves\nprint(\"Unique campaign values:\", train['campaign'].unique())\n\n# Optional: Get value counts to see frequency of each number\nprint(\"Value counts for 'campaign':\")\nprint(train['campaign'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:01:21.353703Z","iopub.execute_input":"2025-08-20T06:01:21.354476Z","iopub.status.idle":"2025-08-20T06:01:21.379447Z","shell.execute_reply.started":"2025-08-20T06:01:21.354444Z","shell.execute_reply":"2025-08-20T06:01:21.378619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check basic stats of 'pdays'\nprint(train['pdays'].describe())\n\n# Number of unique values\nprint(\"Number of unique pdays values:\", train['pdays'].nunique())\n\n# Unique values themselves (optional, might be a lot)\nprint(\"Unique pdays values (sample):\", train['pdays'].unique()[:20])\n\n# Value counts to see frequency of each value\nprint(\"Value counts for 'pdays':\")\nprint(train['pdays'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:02:17.714083Z","iopub.execute_input":"2025-08-20T06:02:17.714351Z","iopub.status.idle":"2025-08-20T06:02:17.755172Z","shell.execute_reply.started":"2025-08-20T06:02:17.714333Z","shell.execute_reply":"2025-08-20T06:02:17.754306Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Number of unique values in 'previous'\nprint(\"Number of unique values in 'previous':\", train['previous'].nunique())\n\n# Display the unique values themselves\nprint(\"Unique 'previous' values:\", train['previous'].unique())\n\n# Value counts to see how often each number occurs\nprint(\"Value counts for 'previous':\")\nprint(train['previous'].value_counts().sort_index())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:03:33.789916Z","iopub.execute_input":"2025-08-20T06:03:33.790252Z","iopub.status.idle":"2025-08-20T06:03:33.812892Z","shell.execute_reply.started":"2025-08-20T06:03:33.790228Z","shell.execute_reply":"2025-08-20T06:03:33.811960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check unique values in 'poutcome'\nprint(\"Unique 'poutcome' values:\", train['poutcome'].unique())\n\n# Number of unique values\nprint(\"Number of unique 'poutcome' values:\", train['poutcome'].nunique())\n\n# Value counts to see distribution\nprint(\"Value counts for 'poutcome':\")\nprint(train['poutcome'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:04:31.496493Z","iopub.execute_input":"2025-08-20T06:04:31.496776Z","iopub.status.idle":"2025-08-20T06:04:31.625980Z","shell.execute_reply.started":"2025-08-20T06:04:31.496750Z","shell.execute_reply":"2025-08-20T06:04:31.625211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform one-hot encoding for 'poutcome' in the training set\npoutcome_dummies_train = pd.get_dummies(train['poutcome'], prefix='poutcome')\n\n# Concatenate the new one-hot columns to the original training dataframe\ntrain = pd.concat([train, poutcome_dummies_train], axis=1)\n\n# Do the same for the test set\npoutcome_dummies_test = pd.get_dummies(test['poutcome'], prefix='poutcome')\ntest = pd.concat([test, poutcome_dummies_test], axis=1)\n\n# Optional: Drop the original 'poutcome' column if you don't need it anymore\ntrain.drop('poutcome', axis=1, inplace=True)\ntest.drop('poutcome', axis=1, inplace=True)\n\n# Check the first few rows to see the one-hot encoded columns\nprint(train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:05:08.261541Z","iopub.execute_input":"2025-08-20T06:05:08.261834Z","iopub.status.idle":"2025-08-20T06:05:08.708098Z","shell.execute_reply.started":"2025-08-20T06:05:08.261804Z","shell.execute_reply":"2025-08-20T06:05:08.707176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:05:22.840928Z","iopub.execute_input":"2025-08-20T06:05:22.841612Z","iopub.status.idle":"2025-08-20T06:05:22.859227Z","shell.execute_reply.started":"2025-08-20T06:05:22.841582Z","shell.execute_reply":"2025-08-20T06:05:22.858270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Columns in the training set\nprint(\"Training set columns:\")\nprint(train.columns)\n\n# Columns in the test set\nprint(\"\\nTest set columns:\")\nprint(test.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:06:30.031595Z","iopub.execute_input":"2025-08-20T06:06:30.031870Z","iopub.status.idle":"2025-08-20T06:06:30.037261Z","shell.execute_reply.started":"2025-08-20T06:06:30.031849Z","shell.execute_reply":"2025-08-20T06:06:30.036313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List all column names in the training dataset\nprint(train.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:08:20.689277Z","iopub.execute_input":"2025-08-20T06:08:20.689558Z","iopub.status.idle":"2025-08-20T06:08:20.694590Z","shell.execute_reply.started":"2025-08-20T06:08:20.689534Z","shell.execute_reply":"2025-08-20T06:08:20.693582Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the original 'month' column since 'month_encoded' exists\ntrain.drop(columns=['month'], inplace=True)\ntest.drop(columns=['month'], inplace=True)\n\n# Check remaining columns\nprint(train.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:10:47.923072Z","iopub.execute_input":"2025-08-20T06:10:47.923665Z","iopub.status.idle":"2025-08-20T06:10:47.983382Z","shell.execute_reply.started":"2025-08-20T06:10:47.923639Z","shell.execute_reply":"2025-08-20T06:10:47.982524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Separate features and target\nX_train = train.drop(columns=['id', 'y'])\ny_train = train['y']\n\n# For test set, drop 'id' column only\nX_test = test.drop(columns=['id'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:13:24.321198Z","iopub.execute_input":"2025-08-20T06:13:24.321909Z","iopub.status.idle":"2025-08-20T06:13:24.569581Z","shell.execute_reply.started":"2025-08-20T06:13:24.321883Z","shell.execute_reply":"2025-08-20T06:13:24.568752Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# List of numeric columns to scale\nnumeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n\n# Initialize the scaler\nscaler = StandardScaler()\n\n# Fit on training data and transform\nX_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n\n# Transform the test set using the same scaler\nX_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n\n# Check first 5 rows after scaling\nprint(X_train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:15:44.814537Z","iopub.execute_input":"2025-08-20T06:15:44.815303Z","iopub.status.idle":"2025-08-20T06:15:45.713107Z","shell.execute_reply.started":"2025-08-20T06:15:44.815277Z","shell.execute_reply":"2025-08-20T06:15:45.712193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\n# Step 1: Initialize the model\nmodel = LogisticRegression(max_iter=1000, random_state=42)\n\n# Step 2: Train the model on the training data\nmodel.fit(X_train, y_train)\n\n# Step 3: Predict probabilities on the training set (to check ROC-AUC)\ny_train_pred_proba = model.predict_proba(X_train)[:, 1]\n\n# Step 4: Calculate ROC-AUC score on training set\nroc_auc = roc_auc_score(y_train, y_train_pred_proba)\nprint(f\"Training ROC-AUC Score: {roc_auc:.4f}\")\n\n# Step 5: Predict probabilities on the test set for submission\ny_test_pred_proba = model.predict_proba(X_test)[:, 1]\n\n# Step 6: Create submission DataFrame\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'y': y_test_pred_proba\n})\n\n# Step 7: Save submission file\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created: submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:18:32.832695Z","iopub.execute_input":"2025-08-20T06:18:32.833813Z","iopub.status.idle":"2025-08-20T06:19:03.146536Z","shell.execute_reply.started":"2025-08-20T06:18:32.833779Z","shell.execute_reply":"2025-08-20T06:19:03.145760Z"}},"outputs":[],"execution_count":null}]}